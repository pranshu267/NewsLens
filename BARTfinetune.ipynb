{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80XMCaJwz047"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets evaluate transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge_score"
      ],
      "metadata": {
        "id": "0AEP7Ngr0HZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BartForConditionalGeneration, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, TrainerCallback\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_2iB8p-Wz4UY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Jb_wLhPp7YU-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"multi_news\")\n",
        "print(dataset)\n",
        "print(f\"Features: {dataset['train'].column_names}\")"
      ],
      "metadata": {
        "id": "DnneqRFs0Qrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "mciCWGDT0dZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch[\"document\"], max_length=1024, truncation=True)\n",
        "    target_encodings = tokenizer(text_target=example_batch[\"summary\"], max_length=256, truncation=True)\n",
        "\n",
        "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
        "           \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "           \"labels\": target_encodings[\"input_ids\"]}\n",
        "\n",
        "new_dataset = dataset.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "3TjMAI0W0kqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "new_dataset.set_format(type=\"torch\", columns=columns)"
      ],
      "metadata": {
        "id": "vKoxLrn20mDU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "pBBvC4ml0s6U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "TpyJuoZ91b00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "h_96KZ5o1ZFJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if \"loss\" in logs or \"eval_loss\" in logs:\n",
        "            self.metrics.append({key: val for key, val in logs.items() if key in [\"loss\", \"eval_loss\"]})\n",
        "\n",
        "metrics_callback = MetricsCallback()"
      ],
      "metadata": {
        "id": "CzOqw56U_Ioj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    output_dir=\"./results\",\n",
        "    logging_steps=50,\n",
        "    eval_steps=50,\n",
        "    gradient_accumulation_steps=4,\n",
        "    load_best_model_at_end=True,\n",
        "    num_train_epochs=3\n",
        ")"
      ],
      "metadata": {
        "id": "Oii-jzQ11f4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=seq2seq_data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=new_dataset['train'],\n",
        "    eval_dataset=new_dataset['validation'],\n",
        "    callbacks=[metrics_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "o5KMm-KX1kOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4uCB9j_36Vt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./results/bert_news_summary/model')"
      ],
      "metadata": {
        "id": "zu-_I_Sh6XRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = [x['loss'] for x in metrics_callback.metrics if 'loss' in x]\n",
        "eval_losses = [x['eval_loss'] for x in metrics_callback.metrics if 'eval_loss' in x]\n",
        "steps = range(0, len(train_losses) * training_args.logging_steps, training_args.logging_steps)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, train_losses, label='Train Loss')\n",
        "if eval_losses:\n",
        "    eval_steps = range(0, len(eval_losses) * training_args.eval_steps, training_args.eval_steps)\n",
        "    plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Evaluation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6rBxDI-g9aIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}